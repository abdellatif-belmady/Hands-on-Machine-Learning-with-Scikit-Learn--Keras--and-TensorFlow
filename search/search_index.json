{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Copyright","text":""},{"location":"#hands-on-machine-learning-with-scikit-learn-keras-and-tensorflow","title":"Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow","text":"<p>by Aur\u00e9lien G\u00e9ron</p> <p>Copyright \u00a9 2019 Aur\u00e9lien G\u00e9ron. All rights reserved.</p> <p>Printed in the United States of America.</p> <p>Published by O\u2019Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.</p> <p>O\u2019Reilly books may be purchased for educational, business, or sales promotional use. Online editions are also available for most titles (http://oreilly.com). For more information, contact our corporate/institutional  sales department: 800-998-9938 or corporate@oreilly.com.</p> <p>Editor: Nicole Tache               |     Cover Designer: Karen Montgomery|</p> <p>Interior Designer: David Futato    |     Illustrator: Rebecca Demarest|</p> <p>June 2019: Second Edition</p> <p>Revision History for the Early Release 2018-11-05: First Release 2019-01-24: Second Release 2019-03-07: Third Release 2019-03-29: Fourth Release 2019-04-22: Fifth Release</p> <p>See http://oreilly.com/catalog/errata.csp?isbn=9781492032649 for release details.</p> <p>The O\u2019Reilly logo is a registered trademark of O\u2019Reilly Media, Inc. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, the cover image, and related trade dress are trademarks of O\u2019Reilly Media, Inc.</p> <p>While the publisher and the author have used good faith efforts to ensure that the information and instructions contained in this work are accurate, the publisher and the author disclaim all responsibility for errors or omissions, including without limitation responsibility for damages resulting from the use of or reliance on this work. Use of the information and instructions contained in this work is at your own risk. If any code samples or other technology this work contains or describes is subject to open source licenses or the intellectual property rights of others, it is your responsibility to ensure that your use thereof complies with such licenses and/or rights.</p>"},{"location":"Neural%20Networks%20and%20Deep%20Learning/4.1.%20Introduction%20to%20Artificial%20Neural%20Networks%20with%20Keras/","title":"Introduction to Artificial Neural Networks with Keras","text":""},{"location":"Neural%20Networks%20and%20Deep%20Learning/4.2.%20Training%20Deep%20Neural%20Networks/","title":"Training Deep Neural Networks","text":""},{"location":"Neural%20Networks%20and%20Deep%20Learning/4.3.%20Custom%20Models%20and%20Training%20with%20TensorFlow/","title":"Custom Models and Training with TensorFlow","text":""},{"location":"Neural%20Networks%20and%20Deep%20Learning/4.4.%20Loading%20and%20Preprocessing%20Data%20with%20TensorFlow/","title":"Custom Models and Training with TensorFlow","text":""},{"location":"Neural%20Networks%20and%20Deep%20Learning/4.5.%20Deep%20Computer%20Vision%20Using%20Convolutional%20Neural%20Networks/","title":"Deep Computer Vision Using Convolutional Neural Networks","text":""},{"location":"Preface/","title":"Preface","text":""},{"location":"Preface/#the-machine-learning-tsunami","title":"The Machine Learning Tsunami","text":"<p>In 2006, Geoffrey Hinton et al. published a paper 1 showing how to train a deep neural network capable of recognizing handwritten digits with state-of-the-art precision (&gt;98%). They branded this technique \u201cDeep Learning.\u201d Training a deep neural net was widely considered impossible at the time,2 and most researchers had abandoned the idea since the 1990s. This paper revived the interest of the scientific community and before long many new papers demonstrated that Deep Learning was not only possible, but capable of mind-blowing achievements that no other Machine Learning (ML) technique could hope to match (with the help of tremendous computing power and great amounts of data). This enthusiasm soon extended to many other areas of Machine Learning.</p> <p>Fast-forward 10 years and Machine Learning has conquered the industry: it is now at the heart of much of the magic in today\u2019s high-tech products, ranking your web search results, powering your smartphone\u2019s speech recognition, recommending videos, and beating the world champion at the game of Go. Before you know it, it will be driving your car.</p>"},{"location":"Preface/#machine-learning-in-your-projects","title":"Machine Learning in Your Projects","text":"<p>So naturally you are excited about Machine Learning and you would love to join the party!</p> <p>Perhaps you would like to give your homemade robot a brain of its own? Make it recognize faces? Or learn to walk around?</p> <p>Or maybe your company has tons of data (user logs, financial data, production data, machine sensor data, hotline stats, HR reports, etc.), and more than likely you could unearth some hidden gems if you just knew where to look; for example:</p> <ul> <li> <p>Segment customers and find the best marketing strategy for each group</p> </li> <li> <p>Recommend products for each client based on what similar clients bought</p> </li> <li> <p>Detect which transactions are likely to be fraudulent</p> </li> <li> <p>Forecast next year\u2019s revenue</p> </li> <li> <p>And more</p> </li> </ul> <p>Whatever the reason, you have decided to learn Machine Learning and implement it in your projects. Great idea!</p>"},{"location":"Preface/#objective-and-approach","title":"Objective and Approach","text":"<p>This book assumes that you know close to nothing about Machine Learning. Its goal is to give you the concepts, the intuitions, and the tools you need to actually implement programs capable of learning from data.</p> <p>We will cover a large number of techniques, from the simplest and most commonly used (such as linear regression) to some of the Deep Learning techniques that regularly win competitions.</p> <p>Rather than implementing our own toy versions of each algorithm, we will be using actual production-ready Python frameworks:</p> <ul> <li> <p>Scikit-Learn is very easy to use, yet it implements many Machine Learning algorithms efficiently, so it makes for a great entry point to learn Machine Learning.</p> </li> <li> <p>TensorFlow is a more complex library for distributed numerical computation. It makes it possible to train and run very large neural networks efficiently by distributing the computations across potentially hundreds of multi-GPU servers. TensorFlow was created at Google and supports many of their large-scale Machine Learning applications. It was open sourced in November 2015.</p> </li> <li> <p>Keras is a high level Deep Learning API that makes it very simple to train and run neural networks. It can run on top of either TensorFlow, Theano or Microsoft Cognitive Toolkit (formerly known as CNTK). TensorFlow comes with its own implementation of this API, called tf.keras, which provides support for some advanced TensorFlow features (e.g., to efficiently load data).</p> </li> </ul> <p>The book favors a hands-on approach, growing an intuitive understanding of Machine Learning through concrete working examples and just a little bit of theory. While you can read this book without picking up your laptop, we highly recommend you experiment with the code examples available online as Jupyter notebooks at https://github.com/ageron/handson-ml2.</p>"},{"location":"Preface/#prerequisites","title":"Prerequisites","text":"<p>This book assumes that you have some Python programming experience and that you are familiar with Python\u2019s main scientific libraries, in particular NumPy, Pandas, and Matplotlib.</p> <p>Also, if you care about what\u2019s under the hood you should have a reasonable understanding of college-level math as well (calculus, linear algebra, probabilities, and statistics).</p> <p>If you don\u2019t know Python yet, http://learnpython.org/ is a great place to start. The official tutorial on python.org is also quite good.</p> <p>If you have never used Jupyter, Chapter 2 will guide you through installation and the basics: it is a great tool to have in your toolbox.</p> <p>If you are not familiar with Python\u2019s scientific libraries, the provided Jupyter notebooks include a few tutorials. There is also a quick math tutorial for linear algebra.</p>"},{"location":"Preface/#roadmap","title":"Roadmap","text":"<p>This book is organized in two parts. The Fundamentals of Machine Learning, covers the following topics:</p> <ul> <li> <p>What is Machine Learning? What problems does it try to solve? What are the main categories and fundamental concepts of Machine Learning systems?</p> </li> <li> <p>The main steps in a typical Machine Learning project.</p> </li> <li> <p>Learning by fitting a model to data.</p> </li> <li> <p>Optimizing a cost function.</p> </li> <li> <p>Handling, cleaning, and preparing data.</p> </li> <li> <p>Selecting and engineering features.</p> </li> <li> <p>The main challenges of Machine Learning, in particular underfitting and overfitting (the bias/variance tradeoff).</p> </li> <li> <p>Reducing the dimensionality of the training data to fight the curse of dimensionality.</p> </li> <li> <p>Other unsupervised learning techniques, including clustering, density estimation and anomaly detection.</p> </li> <li> <p>The most common learning algorithms: Linear and Polynomial Regression, Logistic Regression, k-Nearest Neighbors, Support Vector Machines, Decision Trees, Random Forests, and Ensemble methods.</p> </li> </ul> <p>Neural Networks and Deep Learning, covers the following topics:</p> <ul> <li> <p>What are neural nets? What are they good for?</p> </li> <li> <p>The most important neural net architectures: feedforward neural nets, convolutional nets, recurrent nets, long short-term memory (LSTM) nets, autoencoders and generative adversarial networks (GANs).</p> </li> <li> <p>Techniques for training deep neural nets.</p> </li> <li> <p>Scaling neural networks for large datasets.</p> </li> <li> <p>Learning strategies with Reinforcement Learning.</p> </li> <li> <p>Handling uncertainty with Bayesian Deep Learning.</p> </li> </ul> <p>The first part is based mostly on Scikit-Learn while the second part uses TensorFlow and Keras.</p> <p>Warning</p> <p>Don\u2019t jump into deep waters too hastily: while Deep Learning is no doubt one of the most exciting areas in Machine Learning, you should master the fundamentals first. Moreover, most problems can be solved quite well using simpler techniques such as Random Forests and Ensemble methods (discussed in The Fundamentals of Machine Learning). Deep Learning is best suited for complex problems such as image recognition, speech recognition, or natural language processing, provided you have enough data, computing power, and patience.</p>"},{"location":"Preface/#other-resources","title":"Other Resources","text":"<p>Many resources are available to learn about Machine Learning. Andrew Ng\u2019s ML course on Coursera and Geoffrey Hinton\u2019s course on neural networks and Deep Learning are amazing, although they both require a significant time investment (think months).</p> <p>There are also many interesting websites about Machine Learning, including of course Scikit-Learn\u2019s exceptional User Guide. You may also enjoy Dataquest, which provides very nice interactive tutorials, and ML blogs such as those listed on Quora. Finally, the Deep Learning website has a good list of resources to learn more.</p> <p>Of course there are also many other introductory books about Machine Learning, in particular:</p> <ul> <li> <p>Joel Grus, Data Science from Scratch (O\u2019Reilly). This book presents the fundamentals of Machine Learning, and implements some of the main algorithms in pure Python (from scratch, as the name suggests).</p> </li> <li> <p>Stephen Marsland, Machine Learning: An Algorithmic Perspective (Chapman and Hall). This book is a great introduction to Machine Learning, covering a wide range of topics in depth, with code examples in Python (also from scratch, but using NumPy).</p> </li> <li> <p>Sebastian Raschka, Python Machine Learning (Packt Publishing). Also a great introduction to Machine Learning, this book leverages Python open source libra\u2010 ries (Pylearn 2 and Theano).</p> </li> <li> <p>Fran\u00e7ois Chollet, Deep Learning with Python (Manning). A very practical book that covers a large range of topics in a clear and concise way, as you might expect from the author of the excellent Keras library. It favors code examples over math\u2010 ematical theory.</p> </li> <li> <p>Yaser S. Abu-Mostafa, Malik Magdon-Ismail, and Hsuan-Tien Lin, Learning from Data (AMLBook). A rather theoretical approach to ML, this book provides deep insights, in particular on the bias/variance tradeoff (see Training Models).</p> </li> <li> <p>Stuart Russell and Peter Norvig, Artificial Intelligence: A Modern Approach, 3rd Edition (Pearson). This is a great (and huge) book covering an incredible amount of topics, including Machine Learning. It helps put ML into perspective.</p> </li> </ul> <p>Finally, a great way to learn is to join ML competition websites such as Kaggle.com this will allow you to practice your skills on real-world problems, with help and insights from some of the best ML professionals out there.</p>"},{"location":"Preface/#conventions-used-in-this-book","title":"Conventions Used in This Book","text":"<p>The following typographical conventions are used in this book: Italic     Indicates new terms, URLs, email addresses, filenames, and file extensions. Constant width     Used for program listings, as well as within paragraphs to refer to program elements such as variable or function names, databases, data types, environment variables, statements and keywords. Constant width bold     Shows commands or other text that should be typed literally by the user. Constant width italic     Shows text that should be replaced with user-supplied values or by values determined by context.</p> <p>Tip</p> <p>This element signifies a tip or suggestion.</p> <p>Note</p> <p>This element signifies a general note.</p> <p>Warning</p> <p>This element indicates a warning or caution.</p>"},{"location":"Preface/#code-examples","title":"Code Examples","text":"<p>Supplemental material (code examples, exercises, etc.) is available for download at https://github.com/ageron/handson-ml2. It is mostly composed of Jupyter notebooks.</p> <p>Some of the code examples in the book leave out some repetitive sections, or details that are obvious or unrelated to Machine Learning. This keeps the focus on the important parts of the code, and it saves space to cover more topics. However, if you want the full code examples, they are all available in the Jupyter notebooks.</p> <p>Note that when the code examples display some outputs, then these code examples are shown with Python prompts (&gt;&gt;&gt; and \u2026), as in a Python shell, to clearly distinguish the code from the outputs. For example, this code defines the square() function then it computes and displays the square of 3:</p> <p><pre><code>def square(x):\nreturn x ** 2\n...\nresult = square(3)\nresult\n</code></pre> 9</p> <p>When code does not display anything, prompts are not used. However, the result may sometimes be shown as a comment like this:</p> <pre><code>def square(x):\nreturn x ** 2\nresult = square(3)\nresult # result is 9\n</code></pre>"},{"location":"Preface/#using-code-examples","title":"Using Code Examples","text":"<p>This book is here to help you get your job done. In general, if example code is offered with this book, you may use it in your programs and documentation. You do not need to contact us for permission unless you\u2019re reproducing a significant portion of the code. For example, writing a program that uses several chunks of code from this book does not require permission. Selling or distributing a CD-ROM of examples from O\u2019Reilly books does require permission. Answering a question by citing this book and quoting example code does not require permission. Incorporating a significant amount of example code from this book into your product\u2019s documentation does require permission.</p> <p>We appreciate, but do not require, attribution. An attribution usually includes the title, author, publisher, and ISBN. For example: \u201cHands-On Machine Learning with Scikit-Learn, Keras and TensorFlow by Aur\u00e9lien G\u00e9ron (O\u2019Reilly). Copyright 2019 Aur\u00e9lien G\u00e9ron, 978-1-492-03264-9.\u201d If you feel your use of code examples falls outside fair use or the permission given above, feel free to contact us at permissions@oreilly.com.</p>"},{"location":"Preface/#oreilly-safari","title":"O\u2019Reilly Safari","text":"<p>Safari (formerly Safari Books Online) is a membership-based training and reference platform for enterprise, government, educators, and individuals.</p> <p>Members have access to thousands of books, training videos, Learning Paths, interactive tutorials, and curated playlists from over 250 publishers, including O\u2019Reilly Media, Harvard Business Review, Prentice Hall Professional, Addison-Wesley Professional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal Press, Cisco Press, John Wiley &amp; Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe Press, FT Press, Apress, Manning, New Riders, McGraw-Hill, Jones &amp; Bartlett, and Course Technology, among others.</p> <p>For more information, please visit http://oreilly.com/safari.</p>"},{"location":"Preface/#how-to-contact-us","title":"How to Contact Us","text":"Please address comments and questions concerning this book to the publisher: <p>O\u2019Reilly Media, Inc.</p> <p>1005 Gravenstein Highway North</p> <p>Sebastopol, CA 95472</p> <p>800-998-9938 (in the United States or Canada)</p> <p>707-829-0515 (international or local)</p> <p>707-829-0104 (fax)</p> <p>We have a web page for this book, where we list errata, examples, and any additional information. You can access this page at http://bit.ly/hands-on-machine-learning-with-scikit-learn-and-tensorflow or https://homl.info/oreilly.</p> <p>To comment or ask technical questions about this book, send email to bookques\u2010tions@oreilly.com.</p> <p>For more information about our books, courses, conferences, and news, see our website at http://www.oreilly.com.</p> <p>Find us on Facebook: http://facebook.com/oreilly</p> <p>Follow us on Twitter: http://twitter.com/oreillymedia</p> <p>Watch us on YouTube: http://www.youtube.com/oreillymedia</p>"},{"location":"Preface/#changes-in-the-second-edition","title":"Changes in the Second Edition","text":"<p>This second edition has five main objectives:</p> <ol> <li> <p>Cover additional topics: additional unsupervised learning techniques (including clustering, anomaly detection, density estimation and mixture models), additional techniques for training deep nets (including self-normalized networks), additional computer vision techniques (including the Xception, SENet, object detection with YOLO, and semantic segmentation using R-CNN), handling sequences using CNNs (including WaveNet), natural language processing using RNNs, CNNs and Transformers, generative adversarial networks, deploying TensorFlow models, and more.</p> </li> <li> <p>Update the book to mention some of the latest results from Deep Learning research.</p> </li> <li> <p>Migrate all TensorFlow chapters to TensorFlow 2, and use TensorFlow\u2019s implementation of the Keras API (called tf.keras) whenever possible, to simplify the code examples.</p> </li> <li> <p>Update the code examples to use the latest version of Scikit-Learn, NumPy, Pandas, Matplotlib and other libraries.</p> </li> <li> <p>Clarify some sections and fix some errors, thanks to plenty of great feedback from readers.</p> </li> </ol> <p>Some chapters were added, others were rewritten and a few were reordered. Table P-1 shows the mapping between the 1st edition chapters and the 2nd edition chapters:</p> <p>Table P-1. Chapter mapping between 1st and 2nd edition</p> 1st Ed.chapter 2nd Ed.chapter % Changes 2nd Ed.chapter 1 1 &lt;10% The Machine Learning Landscape 2 2 &lt;10% End-to-End Machine Learning Project 3 3 &lt;10% Classification 4 4 &lt;10% Training Models 5 5 &lt;10% Support Vector Machines 6 6 &lt;10% Decision Trees 7 7 &lt;10% Ensemble Learning and Random Forests 8 8 &lt;10% Dimensionality Reduction N/A 9 100% new Unsupervised Learning Techniques 10 10 ~75% Introduction to Artificial Neural Networks with Keras 11 11 ~50% Training Deep Neural Networks 9 12 100% rewritten Custom Models and Training with TensorFlow Part of 12 13 100% rewritten Loading and Preprocessing Data with TensorFlow 13 14 ~50% Deep Computer Vision Using Convolutional Neural Networks Part of 14 15 ~75% Processing Sequences Using RNNs and CNNs Part of 14 16 ~90% Natural Language Processing with RNNs and Attention 15 17 ~75% Autoencoders and GANs 16 18 ~75% Reinforcement Learning Part of 12 19 100% rewritten Deploying your TensorFlow Models <p>More specifically, here are the main changes for each 2nd edition chapter (other than clarifications, corrections and code updates):</p> <ul> <li> <p>Chapter 1</p> <ul> <li>Added a section on handling mismatch between the training set and the validation &amp; test sets.</li> </ul> </li> <li> <p>Chapter 2</p> <ul> <li>Added how to compute a confidence interval.</li> <li>Improved the installation instructions (e.g., for Windows).</li> <li>Introduced the upgraded OneHotEncoder and the new ColumnTransformer.</li> </ul> </li> <li> <p>Chapter 4</p> <ul> <li>Explained the need for training instances to be Independent and Identically Distributed (IID).</li> </ul> </li> <li> <p>Chapter 7</p> <ul> <li>Added a short section about XGBoost.</li> </ul> </li> <li> <p>Chapter 9 \u2013 new chapter including:</p> <ul> <li>Clustering with K-Means, how to choose the number of clusters, how to use it for dimensionality reduction, semi-supervised learning, image segmentation, and more.</li> <li>Gaussian mixture models, the Expectation-Maximization (EM) algorithm, Bayesian variational inference, and how mixture models can be used for clustering, density estimation, anomaly detection and novelty detection.</li> <li>Overview of other anomaly detection and novelty detection algorithms.</li> </ul> </li> <li> <p>Chapter 10 (mostly new)</p> <ul> <li>Added an introduction to the Keras API, including all its APIs (Sequential, Functional and Subclassing), persistence and callbacks (including the Tensor Board callback).</li> </ul> </li> <li> <p>Chapter 11 (many changes)</p> <ul> <li>Introduced self-normalizing nets, the SELU activation function and Alpha Dropout.</li> <li>Introduced self-supervised learning.</li> <li>Added Nadam optimization.</li> <li>Added Monte-Carlo Dropout.</li> <li>Added a note about the risks of adaptive optimization methods.</li> <li>Updated the practical guidelines.</li> </ul> </li> <li> <p>Chapter 12 \u2013 completely rewritten chapter, including:</p> <ul> <li>A tour of TensorFlow 2</li> <li>TensorFlow\u2019s lower-level Python API</li> <li>Writing custom loss functions, metrics, layers, models</li> <li>Using auto-differentiation and creating custom training algorithms.</li> <li>TensorFlow Functions and graphs (including tracing and autograph).</li> </ul> </li> <li> <p>Chapter 13 \u2013 new chapter, including:</p> <ul> <li>The Data API</li> <li>Loading/Storing data efficiently using TFRecords</li> <li>The Features API (including an introduction to embeddings).</li> <li>An overview of TF Transform and TF Datasets</li> <li>Moved the low-level implementation of the neural network to the exercises.</li> <li>Removed details about queues and readers that are now superseded by the Data API.</li> </ul> </li> <li> <p>Chapter 14</p> <ul> <li>Added Xception and SENet architectures.</li> <li>Added a Keras implementation of ResNet-34.</li> <li>Showed how to use pretrained models using Keras.</li> <li>Added an end-to-end transfer learning example.</li> <li>Added classification and localization.</li> <li>Introduced Fully Convolutional Networks (FCNs).</li> <li>Introduced object detection using the YOLO architecture.</li> <li>Introduced semantic segmentation using R-CNN.</li> </ul> </li> <li> <p>Chapter 15</p> <ul> <li>Added an introduction to Wavenet.</li> <li>Moved the Encoder\u2013Decoder architecture and Bidirectional RNNs to Chapter 16.</li> </ul> </li> <li> <p>Chapter 16</p> <ul> <li>Explained how to use the Data API to handle sequential data.</li> <li>Showed an end-to-end example of text generation using a Character RNN, using both a stateless and a stateful RNN.</li> <li>Showed an end-to-end example of sentiment analysis using an LSTM.</li> <li>Explained masking in Keras.</li> <li>Showed how to reuse pretrained embeddings using TF Hub.</li> <li>Showed how to build an Encoder\u2013Decoder for Neural Machine Translation using TensorFlow Addons/seq2seq.</li> <li>Introduced beam search.</li> <li>Explained attention mechanisms.</li> <li>Added a short overview of visual attention and a note on explainability.</li> <li>Introduced the fully attention-based Transformer architecture, including positional embeddings and multi-head attention.</li> <li>Added an overview of recent language models (2018).</li> </ul> </li> <li> <p>Chapters 17, 18 and 19: coming soon.</p> </li> </ul>"},{"location":"Preface/#acknowledgments","title":"Acknowledgments","text":"<p>Never in my wildest dreams did I imagine that the first edition of this book would get such a large audience. I received so many messages from readers, many asking questions, some kindly pointing out errata, and most sending me encouraging words. I cannot express how grateful I am to all these readers for their tremendous support. Thank you all so very much! Please do not hesitate to file issues on github if you find errors in the code examples (or just to ask questions), or to submit errata if you find errors in the text. Some readers also shared how this book helped them get their first job, or how it helped them solve a concrete problem they were working on: I find such feedback incredibly motivating. If you find this book helpful, I would love it if you could share your story with me, either privately (e.g., via LinkedIn) or publicly (e.g., in an Amazon review).</p> <p>I am also incredibly thankful to all the amazing people who took time out of their busy lives to review my book with such care. In particular, I would like to thank Fran\u00e7ois Chollet for reviewing all the chapters based on Keras &amp; TensorFlow, and giving me some great, in-depth feedback. Since Keras is one of the main additions to this 2nd edition, having its author review the book was invaluable. I highly recommend Fran\u00e7ois\u2019s excellent book Deep Learning with Python3: it has the conciseness, clarity and depth of the Keras library itself. Big thanks as well to Ankur Patel, who reviewed every chapter of this 2nd edition and gave me excellent feedback.</p> <p>This book also benefited from plenty of help from members of the TensorFlow team, in particular Martin Wicke, who tirelessly answered dozens of my questions and dispatched the rest to the right people, including Alexandre Passos, Allen Lavoie, Andr\u00e9 Susano Pinto, Anna Revinskaya, Anthony Platanios, Clemens Mewald, Dan Moldo\u2010van, Daniel Dobson, Dustin Tran, Edd Wilder-James, Goldie Gadde, Jiri Simsa, Karmel Allison, Nick Felt, Paige Bailey, Pete Warden (who also reviewed the 1st edition), Ryan Sepassi, Sandeep Gupta, Sean Morgan, Todd Wang, Tom O\u2019Malley, William Chargin, and Yuefeng Zhou, all of whom were tremendously helpful. A huge thank you to all of you, and to all other members of the TensorFlow team. Not just for your help, but also for making such a great library.</p> <p>Big thanks to Haesun Park, who gave me plenty of excellent feedback and caught several errors while he was writing the Korean translation of the 1st edition of this book. He also translated the Jupyter notebooks to Korean, not to mention TensorFlow\u2019s documentation. I do not speak Korean, but judging by the quality of his feedback, all his translations must be truly excellent! Moreover, he kindly contributed some of the solutions to the exercises in this book.</p> <p>Many thanks as well to O\u2019Reilly\u2019s fantastic staff, in particular Nicole Tache, who gave me insightful feedback, always cheerful, encouraging, and helpful: I could not dream of a better editor. Big thanks to Michele Cronin as well, who was very helpful (and patient) at the start of this 2nd edition. Thanks to Marie Beaugureau, Ben Lorica, Mike Loukides, and Laurel Ruma for believing in this project and helping me define its scope. Thanks to Matt Hacker and all of the Atlas team for answering all my technical questions regarding formatting, asciidoc, and LaTeX, and thanks to Rachel Monaghan, Nick Adams, and all of the production team for their final review and their hundreds of corrections.</p> <p>I would also like to thank my former Google colleagues, in particular the YouTube video classification team, for teaching me so much about Machine Learning. I could never have started the first edition without them. Special thanks to my personal ML gurus: Cl\u00e9ment Courbet, Julien Dubois, Mathias Kende, Daniel Kitachewsky, James Pack, Alexander Pak, Anosh Raj, Vitor Sessak, Wiktor Tomczak, Ingrid von Glehn, Rich Washington, and everyone I worked with at YouTube and in the amazing Google research teams in Mountain View. All these people are just as nice and helpful as they are bright, and that\u2019s saying a lot.</p> <p>I will never forget the kind people who reviewed the 1st edition of this book, including David Andrzejewski, Eddy Hung, Gr\u00e9goire Mesnil, Iain Smears, Ingrid von Glehn, Justin Francis, Karim Matrah, Lukas Biewald, Michel Tessier, Salim S\u00e9maoune, Vincent Guilbeau and of course my dear brother Sylvain.</p> <p>Last but not least, I am infinitely grateful to my beloved wife, Emmanuelle, and to our three wonderful children, Alexandre, R\u00e9mi, and Gabrielle, for encouraging me to work hard on this book, as well as for their insatiable curiosity: explaining some of the most difficult concepts in this book to my wife and children helped me clarify my thoughts and directly improved many parts of this book. Plus, they keep bringing me cookies and coffee! What more can one dream of?</p> <ol> <li> <p>Available on Hinton\u2019s home page at http://www.cs.toronto.edu/~hinton/.\u00a0\u21a9</p> </li> <li> <p>Despite the fact that Yann Lecun\u2019s deep convolutional neural networks had worked well for image recognition since the 1990s, although they were not as general purpose.\u00a0\u21a9</p> </li> <li> <p>\u201cDeep Learning with Python,\u201d Fran\u00e7ois Chollet (2017).\u00a0\u21a9</p> </li> </ol>"},{"location":"The%20Fundamentals%20of%20Machine%20Learning/3.1.%20The%20Machine%20Learning%20Landscape/","title":"The Machine Learning Landscape","text":"<p>Note</p> <p>With Early Release ebooks, you get books in their earliest form the author\u2019s raw and unedited content as he or she writes\u2014so you can take advantage of these technologies long before the official release of these titles. The following will be Chapter 1 in the final release of the book.</p> <p>When most people hear \u201cMachine Learning,\u201d they picture a robot: a dependable but\u2010ler or a deadly Terminator depending on who you ask. But Machine Learning is not just a futuristic fantasy, it\u2019s already here. In fact, it has been around for decades in some specialized applications, such as Optical Character Recognition (OCR). But the first ML application that really became mainstream, improving the lives of hundreds of millions of people, took over the world back in the 1990s: it was the spam filter. Not exactly a self-aware Skynet, but it does technically qualify as Machine Learning (it has actually learned so well that you seldom need to flag an email as spam anymore). It was followed by hundreds of ML applications that now quietly power hundreds of products and features that you use regularly, from better recommendations to voice search.</p> <p>Where does Machine Learning start and where does it end? What exactly does it mean for a machine to learn something? If I download a copy of Wikipedia, has my computer really \u201clearned\u201d something? Is it suddenly smarter? In this chapter we will start by clarifying what Machine Learning is and why you may want to use it.</p> <p>Then, before we set out to explore the Machine Learning continent, we will take a look at the map and learn about the main regions and the most notable landmarks: supervised versus unsupervised learning, online versus batch learning, instance-based versus model-based learning. Then we will look at the workflow of a typical ML project, discuss the main challenges you may face, and cover how to evaluate and fine-tune a Machine Learning system.</p> <p>his chapter introduces a lot of fundamental concepts (and jargon) that every data scientist should know by heart. It will be a high-level overview (the only chapter without much code), all rather simple, but you should make sure everything is crystal-clear to you before continuing to the rest of the book. So grab a coffee and let\u2019s get started!</p> <p>Tip</p> <p>If you already know all the Machine Learning basics, you may want to skip directly to End-to-End Machine Learning Project. If you are not sure, try to answer all the questions listed at the end of the chapter before moving on.</p>"},{"location":"The%20Fundamentals%20of%20Machine%20Learning/3.1.%20The%20Machine%20Learning%20Landscape/#what-is-machine-learning","title":"What Is Machine Learning?","text":"<p>Machine Learning is the science (and art) of programming computers so they can learn from data.</p> <p>Here is a slightly more general definition:</p> <ul> <li> <p>[Machine Learning is the] field of study that gives computers the ability to learn without being explicitly programmed.</p> <ul> <li>Arthur Samuel, 1959</li> </ul> </li> </ul> <p>And a more engineering-oriented one:</p> <ul> <li> <p>A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.</p> <ul> <li>Tom Mitchell, 1997</li> </ul> </li> </ul> <p>For example, your spam filter is a Machine Learning program that can learn to flag spam given examples of spam emails (e.g., flagged by users) and examples of regular (nonspam, also called \u201cham\u201d) emails. The examples that the system uses to learn are called the training set. Each training example is called a training instance (or sample). In this case, the task T is to flag spam for new emails, the experience E is the training data, and the performance measure P needs to be defined; for example, you can use the ratio of correctly classified emails. This particular performance measure is called accuracy and it is often used in classification tasks.</p> <p>If you just download a copy of Wikipedia, your computer has a lot more data, but it is not suddenly better at any task. Thus, it is not Machine Learning.</p>"},{"location":"The%20Fundamentals%20of%20Machine%20Learning/3.1.%20The%20Machine%20Learning%20Landscape/#why-use-machine-learning","title":"Why Use Machine Learning?","text":"<p>Consider how you would write a spam filter using traditional programming techniques (Figure 1-1):</p> <ol> <li> <p>First you would look at what spam typically looks like. You might notice that some words or phrases (such as \u201c4U,\u201d \u201ccredit card,\u201d \u201cfree,\u201d and \u201camazing\u201d) tend to come up a lot in the subject. Perhaps you would also notice a few other patterns in the sender\u2019s name, the email\u2019s body, and so on.</p> </li> <li> <p>You would write a detection algorithm for each of the patterns that you noticed, and your program would flag emails as spam if a number of these patterns are detected.</p> </li> <li> <p>You would test your program, and repeat steps 1 and 2 until it is good enough.</p> </li> </ol> <p> Figure 1-1. The traditional approach</p> <p>Since the problem is not trivial, your program will likely become a long list of complex rules\u2014pretty hard to maintain.</p> <p>In contrast, a spam filter based on Machine Learning techniques automatically learns which words and phrases are good predictors of spam by detecting unusually frequent patterns of words in the spam examples compared to the ham examples (Figure 1-2). The program is much shorter, easier to maintain, and most likely more accurate.</p> <p> Figure 1-2. Machine Learning approach</p> <p>Moreover, if spammers notice that all their emails containing \u201c4U\u201d are blocked, they might start writing \u201cFor U\u201d instead. A spam filter using traditional programming techniques would need to be updated to flag \u201cFor U\u201d emails. If spammers keep working around your spam filter, you will need to keep writing new rules forever.</p> <p>In contrast, a spam filter based on Machine Learning techniques automatically notices that \u201cFor U\u201d has become unusually frequent in spam flagged by users, and it starts flagging them without your intervention (Figure 1-3).</p> <p> Figure 1-3. Automatically adapting to change</p> <p>Another area where Machine Learning shines is for problems that either are too complex for traditional approaches or have no known algorithm. For example, consider speech recognition: say you want to start simple and write a program capable of distinguishing the words \u201cone\u201d and \u201ctwo.\u201d You might notice that the word \u201ctwo\u201d starts with a high-pitch sound (\u201cT\u201d), so you could hardcode an algorithm that measures high-pitch sound intensity and use that to distinguish ones and twos. Obviously this technique will not scale to thousands of words spoken by millions of very different people in noisy environments and in dozens of languages. The best solution (at least today) is to write an algorithm that learns by itself, given many example recordings for each word.</p> <p>Finally, Machine Learning can help humans learn (Figure 1-4): ML algorithms can be inspected to see what they have learned (although for some algorithms this can be tricky). For instance, once the spam filter has been trained on enough spam, it caneasily be inspected to reveal the list of words and combinations of words that it believes are the best predictors of spam. Sometimes this will reveal unsuspected correlations or new trends, and thereby lead to a better understanding of the problem.</p> <p>Applying ML techniques to dig into large amounts of data can help discover patterns that were not immediately apparent. This is called data mining.</p> <p> Figure 1-4. Machine Learning can help humans learn</p> <p>To summarize, Machine Learning is great for:</p> <ul> <li> <p>Problems for which existing solutions require a lot of hand-tuning or long lists of rules: one Machine Learning algorithm can often simplify code and perform better.</p> </li> <li> <p>Complex problems for which there is no good solution at all using a traditional approach: the best Machine Learning techniques can find a solution.</p> </li> <li> <p>Fluctuating environments: a Machine Learning system can adapt to new data.</p> </li> <li> <p>Getting insights about complex problems and large amounts of data.</p> </li> </ul>"},{"location":"The%20Fundamentals%20of%20Machine%20Learning/3.2.%20End-to-End%20Machine%20Learning%20Project/","title":"End-to-End Machine Learning Project","text":""},{"location":"The%20Fundamentals%20of%20Machine%20Learning/3.3.%20Classification/","title":"Classification","text":""},{"location":"The%20Fundamentals%20of%20Machine%20Learning/3.4.%20Training%20Models/","title":"Training Models","text":""},{"location":"The%20Fundamentals%20of%20Machine%20Learning/3.5.%20Support%20Vector%20Machines/","title":"Support Vector Machines","text":""},{"location":"The%20Fundamentals%20of%20Machine%20Learning/3.6.%20Decision%20Trees/","title":"Decision Trees","text":""},{"location":"The%20Fundamentals%20of%20Machine%20Learning/3.7.%20Ensemble%20Learning%20and%20Random%20Forests/","title":"Ensemble Learning and Random Forests","text":""},{"location":"The%20Fundamentals%20of%20Machine%20Learning/3.8.%20Dimensionality%20Reduction/","title":"Dimensionality Reduction","text":""},{"location":"The%20Fundamentals%20of%20Machine%20Learning/3.9.%20Unsupervised%20Learning%20Techniques/","title":"Unsupervised Learning Techniques","text":""}]}